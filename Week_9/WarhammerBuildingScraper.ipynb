{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d370ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # requirements: pip install requests beautifulsoup4 pandas lxml openpyxl\n",
    "# import time, re, csv, json\n",
    "# from urllib.parse import urljoin\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea112e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preflight check: verify required libraries are installed\n",
    "# import importlib\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# REQUIRED = [\n",
    "#     \"requests\",\n",
    "#     \"bs4\",          # BeautifulSoup\n",
    "#     \"pandas\",\n",
    "#     \"lxml\",         # faster HTML parser\n",
    "#     \"openpyxl\"      # Excel writing support for pandas\n",
    "# ]\n",
    "\n",
    "# missing = []\n",
    "# for pkg in REQUIRED:\n",
    "#     try:\n",
    "#         importlib.import_module(pkg)\n",
    "#     except ImportError:\n",
    "#         print(f\"❌ Missing: {pkg}\")\n",
    "#         missing.append(pkg)\n",
    "\n",
    "# if missing:\n",
    "#     print(\"\\nInstalling missing packages...\\n\")\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "# else:\n",
    "#     print(\"✅ All required packages already installed and importable!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18091d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BASE = \"https://totalwarwarhammer.fandom.com\"\n",
    "# INDEX_URL = \"https://totalwarwarhammer.fandom.com/wiki/Immortal_Empires_special_buildings\"\n",
    "\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"IE-special-buildings-scraper (for personal research; contact me if issues)\"\n",
    "# }\n",
    "\n",
    "# SLEEP = 0.7  # be kind to the wiki\n",
    "\n",
    "# def get_soup(url):\n",
    "#     r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "#     r.raise_for_status()\n",
    "#     return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# def find_master_table(soup):\n",
    "#     \"\"\"\n",
    "#     Find the big landmarks table that has headers including:\n",
    "#     Name | Type | Settlement | Province | Region | Starting faction\n",
    "#     \"\"\"\n",
    "#     tables = soup.find_all(\"table\")\n",
    "#     candidate = None\n",
    "#     for t in tables:\n",
    "#         # collect normalized header text\n",
    "#         headers = [th.get_text(\" \", strip=True).lower() for th in t.select(\"thead th\")] \\\n",
    "#                   or [th.get_text(\" \", strip=True).lower() for th in t.select(\"tr th\")]\n",
    "#         head = \" \".join(headers)\n",
    "#         if all(k in head for k in [\"name\", \"type\", \"settlement\", \"province\", \"region\"]):\n",
    "#             candidate = t\n",
    "#             break\n",
    "#     return candidate\n",
    "\n",
    "# def parse_master_table(table):\n",
    "#     rows = []\n",
    "#     # header index map\n",
    "#     header_cells = table.find(\"tr\")\n",
    "#     headers = [h.get_text(\" \", strip=True).lower() for h in header_cells.find_all([\"th\",\"td\"])]\n",
    "#     # build index lookup\n",
    "#     def col_index(name):\n",
    "#         for i, h in enumerate(headers):\n",
    "#             if name in h:\n",
    "#                 return i\n",
    "#         return None\n",
    "\n",
    "#     idx_name = col_index(\"name\")\n",
    "#     idx_type = col_index(\"type\")\n",
    "#     idx_settlement = col_index(\"settlement\")\n",
    "#     idx_province = col_index(\"province\")\n",
    "#     idx_region = col_index(\"region\")\n",
    "#     idx_starting = col_index(\"starting\")\n",
    "\n",
    "#     for tr in table.find_all(\"tr\")[1:]:\n",
    "#         tds = tr.find_all([\"td\",\"th\"])\n",
    "#         if len(tds) < 5: \n",
    "#             continue\n",
    "#         def cell(i):\n",
    "#             return tds[i].get_text(\" \", strip=True) if i is not None and i < len(tds) else \"\"\n",
    "\n",
    "#         # link to building page\n",
    "#         name_cell = tds[idx_name] if idx_name is not None else None\n",
    "#         link = name_cell.find(\"a\") if name_cell else None\n",
    "#         name = (link.get_text(\" \", strip=True) if link else cell(idx_name)).strip()\n",
    "#         url = urljoin(BASE, link[\"href\"]) if link and link.has_attr(\"href\") else \"\"\n",
    "\n",
    "#         rows.append({\n",
    "#             \"building_name\": name,\n",
    "#             \"page_url\": url,\n",
    "#             \"type\": cell(idx_type),\n",
    "#             \"settlement\": cell(idx_settlement),\n",
    "#             \"province\": cell(idx_province),\n",
    "#             \"region\": cell(idx_region),\n",
    "#             \"starting_faction\": cell(idx_starting)\n",
    "#         })\n",
    "#     return rows\n",
    "\n",
    "# def extract_effects_from_building(url):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#       factions (list[str]),\n",
    "#       effects_long (list of dicts: {building_name, faction, effect})\n",
    "#     \"\"\"\n",
    "#     soup = get_soup(url)\n",
    "#     title = soup.select_one(\"h1\")\n",
    "#     building_name = title.get_text(strip=True) if title else url\n",
    "\n",
    "#     # Try to locate the \"Building Effects\" or \"Effects\" section.\n",
    "#     # We will look for any h2 whose text contains 'Effects', then parse following h3 + UL groups until the next h2.\n",
    "#     effects_root = None\n",
    "#     for h2 in soup.select(\"h2\"):\n",
    "#         txt = h2.get_text(\" \", strip=True).lower()\n",
    "#         if \"effects\" in txt:  # matches \"Building Effects\" or \"Effects\"\n",
    "#             effects_root = h2\n",
    "#             break\n",
    "\n",
    "#     factions = []\n",
    "#     long_rows = []\n",
    "#     if effects_root:\n",
    "#         # iterate siblings after h2 until next h2\n",
    "#         for sib in effects_root.find_all_next():\n",
    "#             if sib.name == \"h2\":\n",
    "#                 break\n",
    "#             if sib.name in (\"h3\",\"h4\"):\n",
    "#                 faction = sib.get_text(\" \", strip=True)\n",
    "#                 # skip empty/anchors\n",
    "#                 if not faction or faction.lower() in (\"effects\",):\n",
    "#                     continue\n",
    "#                 factions.append(faction)\n",
    "#                 # collect bullets that immediately follow this heading, until the next heading of same or higher level\n",
    "#                 ul = []\n",
    "#                 cur = sib\n",
    "#                 while True:\n",
    "#                     cur = cur.find_next_sibling()\n",
    "#                     if cur is None: \n",
    "#                         break\n",
    "#                     if cur.name in (\"h2\",\"h3\",\"h4\"):\n",
    "#                         break\n",
    "#                     # bullets can be in <ul> or plain <p> lines\n",
    "#                     for li in cur.select(\"li\"):\n",
    "#                         ul.append(li.get_text(\" \", strip=True))\n",
    "#                     # safety: some pages put single lines in <p>\n",
    "#                     if cur.name == \"p\":\n",
    "#                         text = cur.get_text(\" \", strip=True)\n",
    "#                         if text:\n",
    "#                             ul.append(text)\n",
    "#                 for eff in [e for e in ul if e]:\n",
    "#                     long_rows.append({\n",
    "#                         \"building_name\": building_name,\n",
    "#                         \"page_url\": url,\n",
    "#                         \"faction\": faction,\n",
    "#                         \"effect\": eff\n",
    "#                     })\n",
    "\n",
    "#     # Optional: try to infer province from intro text if the master table missed it\n",
    "#     province_guess = None\n",
    "#     intro = soup.select_one(\"p\")\n",
    "#     if intro:\n",
    "#         text = intro.get_text(\" \", strip=True)\n",
    "#         m = re.search(r\"provincial capital of the ([^.]+)\", text, flags=re.I)\n",
    "#         if m:\n",
    "#             province_guess = m.group(1).strip()\n",
    "\n",
    "#     return building_name, list(dict.fromkeys(factions)), long_rows, province_guess\n",
    "\n",
    "# def main():\n",
    "#     print(\"Fetching index…\")\n",
    "#     index_soup = get_soup(INDEX_URL)\n",
    "#     table = find_master_table(index_soup)\n",
    "#     if table is None:\n",
    "#         raise RuntimeError(\"Could not find the landmarks table on the index page.\")\n",
    "#     master_rows = parse_master_table(table)\n",
    "#     print(f\"Found {len(master_rows)} buildings in the master table.\")\n",
    "\n",
    "#     # Effects per building\n",
    "#     effects_long = []\n",
    "#     factions_map = {}  # building_name -> list of factions\n",
    "#     for i, row in enumerate(master_rows, 1):\n",
    "#         url = row[\"page_url\"]\n",
    "#         if not url:\n",
    "#             continue\n",
    "#         print(f\"[{i}/{len(master_rows)}] {row['building_name']}\")\n",
    "#         try:\n",
    "#             bname, factions, long_rows, province_guess = extract_effects_from_building(url)\n",
    "#             factions_map[bname] = factions\n",
    "#             effects_long.extend(long_rows)\n",
    "#             if not row.get(\"province\") and province_guess:\n",
    "#                 row[\"province\"] = province_guess\n",
    "#         except Exception as e:\n",
    "#             print(\"  -> error:\", e)\n",
    "#         time.sleep(SLEEP)\n",
    "\n",
    "#     # attach factions list to master rows\n",
    "#     for r in master_rows:\n",
    "#         facs = factions_map.get(r[\"building_name\"], [])\n",
    "#         r[\"factions_can_build\"] = \", \".join(facs)\n",
    "\n",
    "#     # Save CSVs\n",
    "#     bld_df = pd.DataFrame(master_rows)\n",
    "#     eff_df = pd.DataFrame(effects_long)\n",
    "\n",
    "#     bld_df.to_csv(\"buildings.csv\", index=False)\n",
    "#     eff_df.to_csv(\"building_effects.csv\", index=False)\n",
    "\n",
    "#     # Save Excel (two sheets)\n",
    "#     with pd.ExcelWriter(\"immortal_empire_buildings.xlsx\", engine=\"openpyxl\") as xw:\n",
    "#         bld_df.to_excel(xw, index=False, sheet_name=\"buildings\")\n",
    "#         eff_df.to_excel(xw, index=False, sheet_name=\"building_effects\")\n",
    "\n",
    "#     # Optional machine-friendly JSON (nested by building -> faction -> effects)\n",
    "#     nested = {}\n",
    "#     for row in master_rows:\n",
    "#         nested[row[\"building_name\"]] = {\n",
    "#             \"page_url\": row[\"page_url\"],\n",
    "#             \"type\": row[\"type\"],\n",
    "#             \"settlement\": row[\"settlement\"],\n",
    "#             \"province\": row[\"province\"],\n",
    "#             \"region\": row[\"region\"],\n",
    "#             \"starting_faction\": row[\"starting_faction\"],\n",
    "#             \"factions\": {}\n",
    "#         }\n",
    "#     for _, g in eff_df.groupby([\"building_name\", \"faction\"]):\n",
    "#         bname = g[\"building_name\"].iloc[0]\n",
    "#         faction = g[\"faction\"].iloc[0]\n",
    "#         nested[bname][\"factions\"].setdefault(faction, [])\n",
    "#         nested[bname][\"factions\"][faction].extend(g[\"effect\"].tolist())\n",
    "\n",
    "#     with open(\"immortal_empire_buildings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(nested, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(\"Done. Wrote buildings.csv, building_effects.csv, immortal_empire_buildings.xlsx, immortal_empire_buildings.json\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bbd476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['tqdm']\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm is installed in '/Users/jamescompagno/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, importlib\n",
    "\n",
    "REQUIRED = [\"requests\", \"bs4\", \"pandas\", \"lxml\", \"openpyxl\", \"tqdm\"]\n",
    "missing = []\n",
    "for pkg in REQUIRED:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(\"Installing:\", missing)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "import os, re, time, json\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Optional: progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **k: x  # fallback: no progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a416924",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://totalwarwarhammer.fandom.com\"\n",
    "INDEX_URL = \"https://totalwarwarhammer.fandom.com/wiki/Immortal_Empires_special_buildings\"\n",
    "HEADERS = {\"User-Agent\": \"IE-special-buildings-scraper (personal research)\"}\n",
    "SLEEP = 0.7  # be kind to the wiki; raise if you hit throttling\n",
    "TIMEOUT = 30\n",
    "RETRIES = 3\n",
    "\n",
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"GET with retries and parser fallback (lxml -> html.parser).\"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            try:\n",
    "                return BeautifulSoup(html, \"lxml\")\n",
    "            except Exception:\n",
    "                return BeautifulSoup(html, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(1.0 * attempt)\n",
    "    raise last_err\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Normalize building names (fixes KeyError from curly quotes/NBSP/etc.).\"\"\"\n",
    "    s = (s or \"\")\n",
    "    s = s.replace(\"\\u00A0\", \" \")\n",
    "    s = s.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def find_master_table(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Find the table that contains headers like:\n",
    "    Name | Type | Settlement | Province | Region | Starting faction\n",
    "    \"\"\"\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for t in tables:\n",
    "        headers = [th.get_text(\" \", strip=True).lower() for th in t.select(\"thead th\")] \\\n",
    "                  or [th.get_text(\" \", strip=True).lower() for th in t.select(\"tr th\")]\n",
    "        if not headers:\n",
    "            continue\n",
    "        head = \" \".join(headers)\n",
    "        if all(k in head for k in (\"name\", \"type\", \"settlement\", \"province\", \"region\")):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def parse_master_table(table) -> list:\n",
    "    rows = []\n",
    "    header_cells = table.find(\"tr\")\n",
    "    headers = [h.get_text(\" \", strip=True).lower() for h in header_cells.find_all([\"th\",\"td\"])]\n",
    "    def col_index(name):\n",
    "        for i, h in enumerate(headers):\n",
    "            if name in h:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    idx_name = col_index(\"name\")\n",
    "    idx_type = col_index(\"type\")\n",
    "    idx_settlement = col_index(\"settlement\")\n",
    "    idx_province = col_index(\"province\")\n",
    "    idx_region = col_index(\"region\")\n",
    "    idx_starting = col_index(\"starting\")  # Starting faction\n",
    "\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        tds = tr.find_all([\"td\",\"th\"])\n",
    "        if len(tds) < 5:\n",
    "            continue\n",
    "\n",
    "        def cell(i):\n",
    "            return tds[i].get_text(\" \", strip=True) if i is not None and i < len(tds) else \"\"\n",
    "\n",
    "        name_cell = tds[idx_name] if idx_name is not None else None\n",
    "        link = name_cell.find(\"a\") if name_cell else None\n",
    "        name = (link.get_text(\" \", strip=True) if link else cell(idx_name)).strip()\n",
    "        url = urljoin(BASE, link[\"href\"]) if link and link.has_attr(\"href\") else \"\"\n",
    "\n",
    "        rows.append({\n",
    "            \"building_name\": name,\n",
    "            \"page_url\": url,\n",
    "            \"type\": cell(idx_type),\n",
    "            \"settlement\": cell(idx_settlement),\n",
    "            \"province\": cell(idx_province),\n",
    "            \"region\": cell(idx_region),\n",
    "            \"starting_faction\": cell(idx_starting),\n",
    "        })\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea3d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_effects_from_building(url: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      building_name (str),\n",
    "      factions (list[str]),\n",
    "      effects_long (list of dicts: {building_name, page_url, faction, effect}),\n",
    "      province_guess (str|None)\n",
    "    \"\"\"\n",
    "    soup = get_soup(url)\n",
    "    title = soup.select_one(\"h1\")\n",
    "    building_name = title.get_text(strip=True) if title else url\n",
    "\n",
    "    # Locate \"Effects\" or \"Building Effects\"\n",
    "    effects_root = None\n",
    "    for h2 in soup.select(\"h2\"):\n",
    "        txt = h2.get_text(\" \", strip=True).lower()\n",
    "        if \"effects\" in txt:\n",
    "            effects_root = h2\n",
    "            break\n",
    "\n",
    "    factions = []\n",
    "    long_rows = []\n",
    "    if effects_root:\n",
    "        node = effects_root\n",
    "        while True:\n",
    "            node = node.find_next_sibling()\n",
    "            if node is None or node.name == \"h2\":\n",
    "                break\n",
    "            # Factions are usually h3/h4 under Effects\n",
    "            if node.name in (\"h3\", \"h4\"):\n",
    "                faction = node.get_text(\" \", strip=True)\n",
    "                if faction and faction.lower() != \"effects\":\n",
    "                    factions.append(faction)\n",
    "                # Collect subsequent bullets/paragraphs until next h3/h4/h2\n",
    "                cursor = node\n",
    "                effects_texts = []\n",
    "                while True:\n",
    "                    cursor = cursor.find_next_sibling()\n",
    "                    if cursor is None or cursor.name in (\"h2\",\"h3\",\"h4\"):\n",
    "                        break\n",
    "                    # bullets\n",
    "                    for li in cursor.select(\"li\"):\n",
    "                        txt = li.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            effects_texts.append(txt)\n",
    "                    # single paragraphs sometimes used\n",
    "                    if cursor.name == \"p\":\n",
    "                        txt = cursor.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            effects_texts.append(txt)\n",
    "                for eff in effects_texts:\n",
    "                    long_rows.append({\n",
    "                        \"building_name\": building_name,\n",
    "                        \"page_url\": url,\n",
    "                        \"faction\": faction if factions else \"\",\n",
    "                        \"effect\": eff\n",
    "                    })\n",
    "\n",
    "    # Optional: try to infer province from intro paragraph if master list missed it\n",
    "    province_guess = None\n",
    "    intro = soup.select_one(\"p\")\n",
    "    if intro:\n",
    "        text = intro.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"provincial capital of the ([^.]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            province_guess = m.group(1).strip()\n",
    "\n",
    "    # de-dup factions while preserving order\n",
    "    seen = set(); uniq_factions = []\n",
    "    for f in factions:\n",
    "        if f not in seen:\n",
    "            uniq_factions.append(f); seen.add(f)\n",
    "\n",
    "    return building_name, uniq_factions, long_rows, province_guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267c0759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching index…\n",
      "Found 288 buildings on the master list.\n",
      "Saved master table -> buildings_master.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching index…\")\n",
    "index_soup = get_soup(INDEX_URL)\n",
    "table = find_master_table(index_soup)\n",
    "if table is None:\n",
    "    raise RuntimeError(\"Could not find the landmarks table on the index page.\")\n",
    "\n",
    "master_rows = parse_master_table(table)\n",
    "print(f\"Found {len(master_rows)} buildings on the master list.\")\n",
    "\n",
    "# Save a checkpoint so you can resume later\n",
    "MASTER_CSV = \"buildings_master.csv\"\n",
    "pd.DataFrame(master_rows).to_csv(MASTER_CSV, index=False)\n",
    "print(f\"Saved master table -> {MASTER_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f907e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping per-building effects…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 282/288 [06:17<00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 98%|█████████▊| 283/288 [06:23<00:14,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 99%|█████████▊| 284/288 [06:30<00:16,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 99%|█████████▉| 285/288 [06:37<00:14,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 99%|█████████▉| 286/288 [06:44<00:10,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|█████████▉| 287/288 [06:50<00:05,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> error on nan : Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [06:57<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved effects -> building_effects.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MASTER_CSV = \"buildings_master.csv\"\n",
    "EFFECTS_PARTIAL = \"building_effects.partial.csv\"\n",
    "EFFECTS_CSV = \"building_effects.csv\"\n",
    "\n",
    "master_df = pd.read_csv(MASTER_CSV)\n",
    "master_rows = master_df.to_dict(\"records\")\n",
    "\n",
    "# Determine which building pages are already scraped (resume)\n",
    "already = set()\n",
    "if os.path.exists(EFFECTS_PARTIAL):\n",
    "    part_df = pd.read_csv(EFFECTS_PARTIAL)\n",
    "    already = set(part_df[\"page_url\"].dropna().tolist())\n",
    "else:\n",
    "    part_df = pd.DataFrame(columns=[\"building_name\",\"page_url\",\"faction\",\"effect\"])\n",
    "    part_df.to_csv(EFFECTS_PARTIAL, index=False)\n",
    "\n",
    "effects_long = []\n",
    "\n",
    "print(\"Scraping per-building effects…\")\n",
    "for row in tqdm(master_rows):\n",
    "    url = str(row.get(\"page_url\") or \"\")\n",
    "    if not url:\n",
    "        continue\n",
    "    if url in already:\n",
    "        continue\n",
    "    try:\n",
    "        bname, factions, long_rows, province_guess = extract_effects_from_building(url)\n",
    "        effects_long.extend(long_rows)\n",
    "        # Update province in-memory if empty and we guessed one\n",
    "        if (not str(row.get(\"province\") or \"\").strip()) and province_guess:\n",
    "            row[\"province\"] = province_guess\n",
    "\n",
    "        # Append incrementally to partial CSV to allow true resume\n",
    "        if long_rows:\n",
    "            pd.DataFrame(long_rows).to_csv(EFFECTS_PARTIAL, mode=\"a\", header=False, index=False)\n",
    "        else:\n",
    "            # still mark as visited\n",
    "            pd.DataFrame([{\"building_name\": bname, \"page_url\": url, \"faction\": \"\", \"effect\": \"\"}]).to_csv(\n",
    "                EFFECTS_PARTIAL, mode=\"a\", header=False, index=False\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\"  -> error on\", url, \":\", e)\n",
    "    time.sleep(SLEEP)\n",
    "\n",
    "# Consolidate (dedupe) partial into final\n",
    "eff_df = pd.read_csv(EFFECTS_PARTIAL)\n",
    "eff_df.drop_duplicates().to_csv(EFFECTS_CSV, index=False)\n",
    "print(f\"Saved effects -> {EFFECTS_CSV}\")\n",
    "\n",
    "# Save updated master with any province guesses we filled\n",
    "pd.DataFrame(master_rows).to_csv(MASTER_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe3b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved buildings -> buildings.csv\n"
     ]
    }
   ],
   "source": [
    "MASTER_CSV = \"buildings_master.csv\"\n",
    "EFFECTS_CSV = \"building_effects.csv\"\n",
    "BUILDINGS_CSV = \"buildings.csv\"\n",
    "\n",
    "master_df = pd.read_csv(MASTER_CSV)\n",
    "eff_df = pd.read_csv(EFFECTS_CSV)\n",
    "\n",
    "# Map building -> factions (from effects section headings)\n",
    "factions_map = {}\n",
    "if not eff_df.empty:\n",
    "    g = eff_df.groupby([\"building_name\", \"faction\"])\n",
    "    for (bname, faction), _ in g:\n",
    "        if pd.isna(faction) or not str(faction).strip():\n",
    "            continue\n",
    "        factions_map.setdefault(bname, set()).add(str(faction).strip())\n",
    "\n",
    "# attach factions list to master rows\n",
    "master_rows = master_df.to_dict(\"records\")\n",
    "for r in master_rows:\n",
    "    facs = sorted(factions_map.get(r[\"building_name\"], []))\n",
    "    r[\"factions_can_build\"] = \", \".join(facs)\n",
    "\n",
    "pd.DataFrame(master_rows).to_csv(BUILDINGS_CSV, index=False)\n",
    "print(f\"Saved buildings -> {BUILDINGS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11181ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel -> immortal_empire_buildings.xlsx\n"
     ]
    }
   ],
   "source": [
    "BUILDINGS_CSV = \"buildings.csv\"\n",
    "EFFECTS_CSV = \"building_effects.csv\"\n",
    "XLSX = \"immortal_empire_buildings.xlsx\"\n",
    "\n",
    "bld_df = pd.read_csv(BUILDINGS_CSV)\n",
    "eff_df = pd.read_csv(EFFECTS_CSV)\n",
    "\n",
    "try:\n",
    "    import openpyxl  # noqa: F401\n",
    "    with pd.ExcelWriter(XLSX, engine=\"openpyxl\") as xw:\n",
    "        bld_df.to_excel(xw, index=False, sheet_name=\"buildings\")\n",
    "        eff_df.to_excel(xw, index=False, sheet_name=\"building_effects\")\n",
    "    print(f\"Saved Excel -> {XLSX}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping Excel export: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe946acb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Seed from master/buildings.csv\u001b[39;00m\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m bld_df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[0;32m---> 18\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mrow_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuilding_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m     nested[k] \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
      "\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactions\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n",
      "\u001b[1;32m     28\u001b[0m     }\n",
      "\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Merge effects; create shells for buildings not in master (prevents KeyError)\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mrow_key\u001b[0;34m(building_name, page_url)\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrow_key\u001b[39m(building_name: \u001b[38;5;28mstr\u001b[39m, page_url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Prefer stable page_url; fallback to normalized name\u001b[39;00m\n",
      "\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m() \u001b[38;5;129;01mor\u001b[39;00m norm_name(building_name)\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "BUILDINGS_CSV = \"buildings.csv\"\n",
    "EFFECTS_CSV = \"building_effects.csv\"\n",
    "JSON_OUT = \"immortal_empire_buildings.json\"\n",
    "\n",
    "bld_df = pd.read_csv(BUILDINGS_CSV)\n",
    "eff_df = pd.read_csv(EFFECTS_CSV) if os.path.exists(EFFECTS_CSV) else pd.DataFrame(\n",
    "    columns=[\"building_name\",\"page_url\",\"faction\",\"effect\"]\n",
    ")\n",
    "\n",
    "def row_key(building_name: str, page_url: str) -> str:\n",
    "    # Prefer stable page_url; fallback to normalized name\n",
    "    return (page_url or \"\").strip() or norm_name(building_name)\n",
    "\n",
    "nested = {}\n",
    "\n",
    "# Seed from master/buildings.csv\n",
    "for _, r in bld_df.iterrows():\n",
    "    k = row_key(r.get(\"building_name\",\"\"), r.get(\"page_url\",\"\"))\n",
    "    nested[k] = {\n",
    "        \"building_name\": r.get(\"building_name\",\"\"),\n",
    "        \"page_url\": r.get(\"page_url\",\"\"),\n",
    "        \"type\": r.get(\"type\",\"\"),\n",
    "        \"settlement\": r.get(\"settlement\",\"\"),\n",
    "        \"province\": r.get(\"province\",\"\"),\n",
    "        \"region\": r.get(\"region\",\"\"),\n",
    "        \"starting_faction\": r.get(\"starting_faction\",\"\"),\n",
    "        \"factions\": {}\n",
    "    }\n",
    "\n",
    "# Merge effects; create shells for buildings not in master (prevents KeyError)\n",
    "orphan_count = 0\n",
    "if not eff_df.empty:\n",
    "    if \"page_url\" not in eff_df.columns:\n",
    "        eff_df[\"page_url\"] = \"\"\n",
    "    for (_, faction), g in eff_df.groupby([\"building_name\",\"faction\"], dropna=False):\n",
    "        bname = str(g[\"building_name\"].iloc[0])\n",
    "        page = str(g[\"page_url\"].iloc[0]) if \"page_url\" in g.columns else \"\"\n",
    "        k = row_key(bname, page)\n",
    "        if k not in nested:\n",
    "            orphan_count += 1\n",
    "            print(f\"⚠️ Creating shell for orphaned building: {bname}\")\n",
    "            nested[k] = {\n",
    "                \"building_name\": bname,\n",
    "                \"page_url\": page,\n",
    "                \"type\": \"\",\n",
    "                \"settlement\": \"\",\n",
    "                \"province\": \"\",\n",
    "                \"region\": \"\",\n",
    "                \"starting_faction\": \"\",\n",
    "                \"factions\": {}\n",
    "            }\n",
    "        nested[k][\"factions\"].setdefault(str(faction or \"\"), [])\n",
    "        nested[k][\"factions\"][str(faction or \"\")].extend(\n",
    "            [e for e in g[\"effect\"].dropna().astype(str).tolist() if e.strip()]\n",
    "        )\n",
    "\n",
    "with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nested, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved JSON -> {JSON_OUT}\")\n",
    "if orphan_count:\n",
    "    print(f\"ℹ️ Note: {orphan_count} building(s) had effects but weren’t in the master table; shells created in JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
