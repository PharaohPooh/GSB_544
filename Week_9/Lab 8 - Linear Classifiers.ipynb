{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfdc3ea",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lab 8: Linear Classifiers\"\n",
    "format: \n",
    "  html:\n",
    "    embed-resources: true\n",
    "execute:\n",
    "  echo: true\n",
    "code-fold: true\n",
    "author: James Compagno\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d76738",
   "metadata": {},
   "source": [
    "# The Data\n",
    "This week, we consider a dataset generated from text data.\n",
    "\n",
    "The original dataset can be found here: https://www.kaggle.com/datasets/kingburrito666/cannabis-strains. It consists of user reviews of different strains of cannabis. Users rated their experience with the cannabis strain on a scale of 1 to 5. They also selected words from a long list to describe the Effects and the Flavor of the cannabis.\n",
    "\n",
    "In the dataset linked above, each row is one strain of cannabis. The average rating of all testers is reported, as well as the most commonly used words for the effect and flavor.\n",
    "\n",
    "Some data cleaning has been performed for you: The Effect and Flavor columns have been converted to dummy variables indicating if the particular word was used for the particular strain.\n",
    "\n",
    "This cleaned data can be found at: https://www.dropbox.com/s/s2a1uoiegitupjc/cannabis_full.csv\n",
    "Our goal will be to fit models that identify the Sativa types from the Indica types, and then to fit models that also distinguish the Hybrid types.\n",
    "\n",
    "IMPORTANT: In this assignment, you do not need to consider different feature sets. Normally, this would be a good thing to try - but for this homework, simply include all the predictors for every model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfaf9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, \n",
    "                             precision_recall_fscore_support, roc_auc_score, \n",
    "                             confusion_matrix, classification_report, roc_curve, \n",
    "                             auc, precision_score, recall_score, f1_score)\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d37f4c",
   "metadata": {},
   "source": [
    "# Part One: Binary Classification\n",
    "\n",
    "Create a dataset that is limited only to the Sativa and Indica type cannabis strains.\n",
    "\n",
    "This section asks you to create a final best model for each of the four new model types studied this week: LDA, QDA, SVC, and SVM. For SVM, you may limit yourself to only the polynomial kernel.\n",
    "\n",
    "For each, you should:\n",
    "\n",
    "    - Choose a metric you will use to select your model, and briefly justify your choice. (Hint: There is no specific target category here, so this should not be a metric that only prioritizes one category.)\n",
    "\n",
    "    - Find the best model for predicting the Type variable. Don't forget to tune any hyperparameters. \n",
    "\n",
    "    - Report the (cross-validated!) metric.\n",
    "    \n",
    "    - Fit the final model.\n",
    "    \n",
    "    - Output a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee2b11",
   "metadata": {},
   "source": [
    "For my metric I will choose ROC-AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "155400d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Energetic</th>\n",
       "      <th>Tingly</th>\n",
       "      <th>Euphoric</th>\n",
       "      <th>Relaxed</th>\n",
       "      <th>Aroused</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Uplifted</th>\n",
       "      <th>Hungry</th>\n",
       "      <th>Talkative</th>\n",
       "      <th>Giggly</th>\n",
       "      <th>Focused</th>\n",
       "      <th>Sleepy</th>\n",
       "      <th>Dry</th>\n",
       "      <th>Mouth</th>\n",
       "      <th>Earthy</th>\n",
       "      <th>Sweet</th>\n",
       "      <th>Citrus</th>\n",
       "      <th>Flowery</th>\n",
       "      <th>Violet</th>\n",
       "      <th>Diesel</th>\n",
       "      <th>Spicy/Herbal</th>\n",
       "      <th>Sage</th>\n",
       "      <th>Woody</th>\n",
       "      <th>Apricot</th>\n",
       "      <th>Grapefruit</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Pungent</th>\n",
       "      <th>Grape</th>\n",
       "      <th>Pine</th>\n",
       "      <th>Skunk</th>\n",
       "      <th>Berry</th>\n",
       "      <th>Pepper</th>\n",
       "      <th>Menthol</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Cheese</th>\n",
       "      <th>Chemical</th>\n",
       "      <th>Mango</th>\n",
       "      <th>Lemon</th>\n",
       "      <th>Peach</th>\n",
       "      <th>Vanilla</th>\n",
       "      <th>Nutty</th>\n",
       "      <th>Chestnut</th>\n",
       "      <th>Tea</th>\n",
       "      <th>Tobacco</th>\n",
       "      <th>Tropical</th>\n",
       "      <th>Strawberry</th>\n",
       "      <th>Blueberry</th>\n",
       "      <th>Mint</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Honey</th>\n",
       "      <th>Lavender</th>\n",
       "      <th>Lime</th>\n",
       "      <th>Coffee</th>\n",
       "      <th>Ammonia</th>\n",
       "      <th>Minty</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Butter</th>\n",
       "      <th>Pineapple</th>\n",
       "      <th>Tar</th>\n",
       "      <th>Rose</th>\n",
       "      <th>Plum</th>\n",
       "      <th>Pear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.430169</td>\n",
       "      <td>0.329986</td>\n",
       "      <td>0.283432</td>\n",
       "      <td>0.151073</td>\n",
       "      <td>0.727522</td>\n",
       "      <td>0.772250</td>\n",
       "      <td>0.088088</td>\n",
       "      <td>0.835691</td>\n",
       "      <td>0.670927</td>\n",
       "      <td>0.208581</td>\n",
       "      <td>0.158832</td>\n",
       "      <td>0.130534</td>\n",
       "      <td>0.264263</td>\n",
       "      <td>0.327704</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.504336</td>\n",
       "      <td>0.480146</td>\n",
       "      <td>0.240073</td>\n",
       "      <td>0.121406</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.109539</td>\n",
       "      <td>0.102693</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.116385</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.035144</td>\n",
       "      <td>0.205842</td>\n",
       "      <td>0.074395</td>\n",
       "      <td>0.154267</td>\n",
       "      <td>0.079416</td>\n",
       "      <td>0.162026</td>\n",
       "      <td>0.026472</td>\n",
       "      <td>0.010497</td>\n",
       "      <td>0.069375</td>\n",
       "      <td>0.029210</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.014605</td>\n",
       "      <td>0.086262</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.066180</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.024190</td>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.001369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.419576</td>\n",
       "      <td>0.470315</td>\n",
       "      <td>0.450767</td>\n",
       "      <td>0.358201</td>\n",
       "      <td>0.445336</td>\n",
       "      <td>0.419476</td>\n",
       "      <td>0.283487</td>\n",
       "      <td>0.370640</td>\n",
       "      <td>0.469984</td>\n",
       "      <td>0.406387</td>\n",
       "      <td>0.365602</td>\n",
       "      <td>0.336967</td>\n",
       "      <td>0.441041</td>\n",
       "      <td>0.469484</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.500095</td>\n",
       "      <td>0.499720</td>\n",
       "      <td>0.427225</td>\n",
       "      <td>0.326673</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.312386</td>\n",
       "      <td>0.303627</td>\n",
       "      <td>0.132254</td>\n",
       "      <td>0.320759</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>0.130578</td>\n",
       "      <td>0.184185</td>\n",
       "      <td>0.404408</td>\n",
       "      <td>0.262473</td>\n",
       "      <td>0.361287</td>\n",
       "      <td>0.270448</td>\n",
       "      <td>0.368559</td>\n",
       "      <td>0.160571</td>\n",
       "      <td>0.101941</td>\n",
       "      <td>0.254148</td>\n",
       "      <td>0.168434</td>\n",
       "      <td>0.128878</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.280815</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>0.123629</td>\n",
       "      <td>0.106232</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.087763</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>0.254920</td>\n",
       "      <td>0.144917</td>\n",
       "      <td>0.248653</td>\n",
       "      <td>0.155080</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>0.118131</td>\n",
       "      <td>0.128878</td>\n",
       "      <td>0.153673</td>\n",
       "      <td>0.104110</td>\n",
       "      <td>0.112348</td>\n",
       "      <td>0.135540</td>\n",
       "      <td>0.123629</td>\n",
       "      <td>0.123629</td>\n",
       "      <td>0.092739</td>\n",
       "      <td>0.137151</td>\n",
       "      <td>0.060329</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.036986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating     Creative  ...         Plum         Pear\n",
       "count  2191.000000  2191.000000  ...  2191.000000  2191.000000\n",
       "mean      4.430169     0.329986  ...     0.000913     0.001369\n",
       "std       0.419576     0.470315  ...     0.030206     0.036986\n",
       "min       0.000000     0.000000  ...     0.000000     0.000000\n",
       "25%       4.300000     0.000000  ...     0.000000     0.000000\n",
       "50%       4.400000     0.000000  ...     0.000000     0.000000\n",
       "75%       4.600000     1.000000  ...     0.000000     0.000000\n",
       "max       5.000000     1.000000  ...     1.000000     1.000000\n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weed = pd.read_csv(\"https://www.dropbox.com/s/s2a1uoiegitupjc/cannabis_full.csv?dl=1\")\n",
    "weed = weed.dropna()\n",
    "\n",
    "weed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b326423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in weed.columns:\n",
    "    if col not in ['Type', 'Strain', 'Effects', 'Flavor']:\n",
    "        weed[col] = pd.to_numeric(weed[col], errors='coerce')\n",
    "\n",
    "# Then proceed with your normal setup\n",
    "y = weed['Type']\n",
    "X = weed.drop(columns=['Type', 'Strain', 'Effects', 'Flavor'])\n",
    "\n",
    "# Binary Split \n",
    "binary_weed = weed['Type'].isin(['indica', 'sativa'])\n",
    "X_binary = X[binary_weed] \n",
    "y_binary = y[binary_weed]\n",
    "\n",
    "# Model Library \n",
    "model_library = {}\n",
    "records = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6726725",
   "metadata": {},
   "source": [
    "## Q1: LDA - Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91b80218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (CV):\n",
      "[[597  62]\n",
      " [ 88 321]]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LDA_Binary\"\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Cross-validated prediction\n",
    "y_pred_cv = cross_val_predict(lda_model, X_binary, y_binary, cv=5)\n",
    "y_proba_cv = cross_val_predict(lda_model, X_binary, y_binary, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Metrics\n",
    "conf_matrix = confusion_matrix(y_binary, y_pred_cv)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "cv_roc_auc = roc_auc_score(y_binary, y_proba_cv)\n",
    "cv_accuracy = accuracy_score(y_binary, y_pred_cv)\n",
    "precision = precision_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "recall = recall_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "# Fit \n",
    "lda_model.fit(X_binary, y_binary)\n",
    "\n",
    "# Store in model library\n",
    "model_library[model_name] = lda_model\n",
    "\n",
    "# Store results \n",
    "records.append({\n",
    "    \"Model\": model_name,\n",
    "    \"Classification Type\": \"LDA\",\n",
    "    \"Variables Used\": \"All\",\n",
    "    \"Hyperparameter 1 Name\": \"NA\", \n",
    "    \"Hyperparameter 1 Value\": \"NA\",\n",
    "    \"Hyperparameter 2 Name\": \"NA\", \n",
    "    \"Hyperparameter 2 Value\": \"NA\",\n",
    "    \"Range Tested\": \"NA\",\n",
    "    \"ROC AUC\": cv_roc_auc,\n",
    "    \"CV Accuracy\": cv_accuracy,\n",
    "    \"Confusion Matrix\": conf_matrix,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"Specificity\": specificity,\n",
    "})\n",
    "\n",
    "# Print\n",
    "print(\"Confusion Matrix (CV):\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc707d87",
   "metadata": {},
   "source": [
    "The LDA model is overall strong at identifying the correct strain with a ROC-AUC of 0.932. However, it had a lower Recall of (0.785) so some sativa strains were miss-classified. The model was very good however at classifying indica strains with a specificity of Specificity 0.906. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8a694",
   "metadata": {},
   "source": [
    "## Q2: QDA - Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "771dfab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (CV):\n",
      "[[601  58]\n",
      " [ 92 317]]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"QDA_Binary\"\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Parameter grid for QDA \n",
    "param_grid = {\n",
    "    'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    qda_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_binary, y_binary)\n",
    "\n",
    "# Best model\n",
    "best_qda = grid_search.best_estimator_\n",
    "best_reg_param = grid_search.best_params_['reg_param']\n",
    "\n",
    "# Cross-validated prediction with best model\n",
    "y_pred_cv = cross_val_predict(best_qda, X_binary, y_binary, cv=5)\n",
    "y_proba_cv = cross_val_predict(best_qda, X_binary, y_binary, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Metrics\n",
    "conf_matrix = confusion_matrix(y_binary, y_pred_cv)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "cv_roc_auc = roc_auc_score(y_binary, y_proba_cv)\n",
    "cv_accuracy = accuracy_score(y_binary, y_pred_cv)\n",
    "precision = precision_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "recall = recall_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "# Store in model library\n",
    "model_library[model_name] = best_qda\n",
    "\n",
    "# Store results \n",
    "records.append({\n",
    "    \"Model\": model_name,\n",
    "    \"Classification Type\": \"QDA\",\n",
    "    \"Variables Used\": \"All\",\n",
    "    \"Hyperparameter 1 Name\": \"reg_param\", \n",
    "    \"Hyperparameter 1 Value\": best_reg_param,\n",
    "    \"Hyperparameter 2 Name\": \"NA\", \n",
    "    \"Hyperparameter 2 Value\": \"NA\",\n",
    "    \"Range Tested\": str(param_grid['reg_param']),\n",
    "    \"ROC AUC\": cv_roc_auc,\n",
    "    \"CV Accuracy\": cv_accuracy,\n",
    "    \"Confusion Matrix\": conf_matrix,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"Specificity\": specificity,\n",
    "})\n",
    "\n",
    "# Print\n",
    "print(\"Confusion Matrix (CV):\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b6d96",
   "metadata": {},
   "source": [
    "With a ROC AUC of 0.937 QDA slightly out performs LDA overall. The QDA correctly identified 4 more indica strains than LDA at the expense of misclassfiying 4 staiva strains as indica. Percision therefore went up at the cost of recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f171da9",
   "metadata": {},
   "source": [
    "## Q3: SVC - Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63703747",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SVC_Binary\"\n",
    "svc_model = SVC(kernel='linear', probability=True, random_state=67)\n",
    "\n",
    "# Parameter grid for SVC\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    svc_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_binary, y_binary)\n",
    "\n",
    "# Best model\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "# Cross-validated prediction with best model\n",
    "y_pred_cv = cross_val_predict(best_svc, X_binary, y_binary, cv=5)\n",
    "y_proba_cv = cross_val_predict(best_svc, X_binary, y_binary, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Metrics\n",
    "conf_matrix = confusion_matrix(y_binary, y_pred_cv)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "cv_roc_auc = roc_auc_score(y_binary, y_proba_cv)\n",
    "cv_accuracy = accuracy_score(y_binary, y_pred_cv)\n",
    "precision = precision_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "recall = recall_score(y_binary, y_pred_cv, pos_label='sativa', zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "# Store in model library\n",
    "model_library[model_name] = best_svc\n",
    "\n",
    "# Store results \n",
    "records.append({\n",
    "    \"Model\": model_name,\n",
    "    \"Classification Type\": \"SVC\",\n",
    "    \"Variables Used\": \"All\",\n",
    "    \"Hyperparameter 1 Name\": \"C\", \n",
    "    \"Hyperparameter 1 Value\": best_C,\n",
    "    \"Hyperparameter 2 Name\": \"NA\", \n",
    "    \"Hyperparameter 2 Value\": \"NA\",\n",
    "    \"Range Tested\": str(param_grid['C']),\n",
    "    \"ROC AUC\": cv_roc_auc,\n",
    "    \"CV Accuracy\": cv_accuracy,\n",
    "    \"Confusion Matrix\": conf_matrix,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"Specificity\": specificity,\n",
    "})\n",
    "\n",
    "# Print\n",
    "print(\"Confusion Matrix (CV):\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c0601",
   "metadata": {},
   "source": [
    "## Q4: SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcc90f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Classification Type</th>\n",
       "      <th>Variables Used</th>\n",
       "      <th>Hyperparameter 1 Name</th>\n",
       "      <th>Hyperparameter 1 Value</th>\n",
       "      <th>Hyperparameter 2 Name</th>\n",
       "      <th>Hyperparameter 2 Value</th>\n",
       "      <th>Range Tested</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QDA_Binary</td>\n",
       "      <td>QDA</td>\n",
       "      <td>All</td>\n",
       "      <td>reg_param</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, ...</td>\n",
       "      <td>0.937465</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>[[601, 58], [92, 317]]</td>\n",
       "      <td>0.845333</td>\n",
       "      <td>0.775061</td>\n",
       "      <td>0.911988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA_Binary</td>\n",
       "      <td>LDA</td>\n",
       "      <td>All</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.931704</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>[[597, 62], [88, 321]]</td>\n",
       "      <td>0.838120</td>\n",
       "      <td>0.784841</td>\n",
       "      <td>0.905918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model Classification Type  ...    Recall Specificity\n",
       "1  QDA_Binary                 QDA  ...  0.775061    0.911988\n",
       "0  LDA_Binary                 LDA  ...  0.784841    0.905918\n",
       "\n",
       "[2 rows x 14 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPt1 = pd.DataFrame(records)\n",
    "dfPt1.sort_values('ROC AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec849e",
   "metadata": {},
   "source": [
    "# Part Two: Natural Multiclass\n",
    "Now use the full dataset, including the Hybrid strains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdecc7",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Fit a decision tree, plot the final fit, and interpret the results.\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d235d5",
   "metadata": {},
   "source": [
    "## Q2\n",
    "Repeat the analyses from Part One for LDA, QDA, and KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1606bb",
   "metadata": {},
   "source": [
    "## Q3\n",
    "Were your metrics better or worse than in Part One? Why? Which categories were most likely to get mixed up, according to the confusion matrices? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e08a6b",
   "metadata": {},
   "source": [
    "# Part Three: Multiclass from Binary\n",
    "Consider two models designed for binary classification: SVC and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236f166",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Fit and report metrics for OvR versions of the models. That is, for each of the two model types, create three models:\n",
    "\n",
    "    - Indica vs. Not Indica\n",
    "    - Sativa vs. Not Sativa\n",
    "    - Hybrid vs. Not Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61a027",
   "metadata": {},
   "source": [
    "## Q2\n",
    "Which of the six models did the best job distinguishing the target category from the rest? Which did the worst? Does this make intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9315f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "Fit and report metrics for OvO versions of the models. That is, for each of the two model types, create three models:\n",
    "\n",
    "    - Indica vs. Sativa\n",
    "    - Indica vs. Hybrid\n",
    "    - Hybrid vs. Sativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f799f2",
   "metadata": {},
   "source": [
    "## Q4\n",
    "Which of the six models did the best job distinguishing at differentiating the two groups? Which did the worst? Does this make intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9c80b",
   "metadata": {},
   "source": [
    "## Q5\n",
    "Suppose you had simply input the full data, with three classes, into the LogisticRegression function. Would this have automatically taken an \"OvO\" approach or an \"OvR\" approach?\n",
    "\n",
    "What about for SVC?\n",
    "\n",
    "Note: You do not actually have to run code here - you only need to look at sklearn's documentation to see how these functions handle multiclass input."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
